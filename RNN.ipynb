{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8eBqscKBkKC"
      },
      "source": [
        "# **Implementing Many-to-Many RNN for English-to-Urdu Language Translation and Exploring Its Limitations**\n",
        "\n",
        "# **Part 1:** Many-to-Many Recurrent Neural Network (RNN) Implementation\n",
        "\n",
        "## Data Preparation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dfQ_cl5TBkKG",
        "outputId": "8d1ac82b-fbe7-4b54-9d55-f1c636b12be9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_excel('./parallel-corpus.xlsx')\n",
        "\n",
        "# Keep only the first two columns\n",
        "df = df.iloc[:, :2]\n",
        "\n",
        "df.rename(columns = {'SENTENCES ':'SENTENCES'}, inplace = True)\n",
        "\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy1yA38HBkKI"
      },
      "source": [
        "Step 1: Preprocess the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heK7hl3DBkKJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer_eng = Tokenizer()\n",
        "tokenizer_urdu = Tokenizer()\n",
        "\n",
        "# Convert the 'SENTENCES' column to string type before fitting the tokenizer\n",
        "df['SENTENCES'] = df['SENTENCES'].astype(str)\n",
        "# Convert the 'MEANING' column to string type before fitting the tokenizer\n",
        "df['MEANING'] = df['MEANING'].astype(str)\n",
        "\n",
        "tokenizer_eng.fit_on_texts(df['SENTENCES'])\n",
        "tokenizer_urdu.fit_on_texts(df['MEANING'])\n",
        "\n",
        "eng_sequences = tokenizer_eng.texts_to_sequences(df['SENTENCES'])\n",
        "urdu_sequences = tokenizer_urdu.texts_to_sequences(df['MEANING'])\n",
        "\n",
        "# Pad sequences\n",
        "max_len_eng = max(len(seq) for seq in eng_sequences)\n",
        "max_len_urdu = max(len(seq) for seq in urdu_sequences)\n",
        "\n",
        "max_len = max(max_len_eng,max_len_urdu)\n",
        "\n",
        "eng_sequences = pad_sequences(eng_sequences, maxlen=max_len, padding='post')\n",
        "urdu_sequences = pad_sequences(urdu_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Vocabulary sizes\n",
        "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
        "vocab_size_urdu = len(tokenizer_urdu.word_index) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac_QVU3wBkKK"
      },
      "source": [
        "Step 2: Prepare the Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rmfxvpX9BkKK"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and validation sets\n",
        "train_size = int(len(eng_sequences) * 0.7)\n",
        "test_size = int(len(eng_sequences) * 0.15)\n",
        "\n",
        "# For English train, validation and test\n",
        "x_train, x_temp = eng_sequences[:train_size], eng_sequences[train_size:]\n",
        "x_test, x_val = x_temp[:test_size], x_temp[test_size:]\n",
        "\n",
        "# For Urdu train, validation and test\n",
        "y_train, y_temp = urdu_sequences[:train_size], urdu_sequences[train_size:]\n",
        "y_test, y_val = y_temp[:test_size], y_temp[test_size:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(x_train.shape, y_train.shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4WkDPvwBkKL"
      },
      "source": [
        "Step 3: Build the RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "pZII0cIeBkKL",
        "outputId": "4e8af732-0fd6-4194-aee4-cd379c3d6caa"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN, Dense\n",
        "from keras_nlp.metrics import Bleu\n",
        "\n",
        "\n",
        "# Build Model\n",
        "model = Sequential(\n",
        "    [\n",
        "        Embedding(vocab_size_eng, 64,),\n",
        "        SimpleRNN(64, return_sequences=True),\n",
        "        Dense(vocab_size_urdu, activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile Model\n",
        "bleu_metric = Bleu(tokenizer=tokenizer_urdu.texts_to_sequences)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[bleu_metric])\n",
        "\n",
        "# Train Model\n",
        "model.fit(x_train, y_train, epochs=100,validation_data=(x_val,y_val))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.evaluate(x_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translate function\n",
        "def translate(text):\n",
        "    sequence = tokenizer_eng.texts_to_sequences([text])\n",
        "    sequence = pad_sequences(sequence, maxlen=max_len_eng, padding='post')\n",
        "    prediction = model.predict(sequence)\n",
        "    predicted_sequence = np.argmax(prediction, axis=-1)\n",
        "    translated_text = ' '.join([tokenizer_urdu.index_word[idx] for idx in predicted_sequence[0] if idx != 0])\n",
        "    return translated_text\n",
        "\n",
        "# Example translation\n",
        "print(translate('hello'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
