{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementing Many-to-Many RNN for English-to-Urdu Language Translation and Exploring Its Limitations**\n",
    "\n",
    "# **Part 1:** Many-to-Many Recurrent Neural Network (RNN) Implementation\n",
    "\n",
    "## Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCES</th>\n",
       "      <th>MEANING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I communicate with my parents?</td>\n",
       "      <td>میں اپنے والدین سے کیسے بات کروں ؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I make friends?’</td>\n",
       "      <td>میں دوست کیسے بنائوں ؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do I get so sad?’</td>\n",
       "      <td>میں اتنا اداس کیوں ہوں؟.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you’ve asked yourself such questions, you’r...</td>\n",
       "      <td>اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Depending on where you’ve turned for guidance,...</td>\n",
       "      <td>اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          SENTENCES   \\\n",
       "0             How can I communicate with my parents?   \n",
       "1                           How can I make friends?’   \n",
       "2                              Why do I get so sad?’   \n",
       "3  If you’ve asked yourself such questions, you’r...   \n",
       "4  Depending on where you’ve turned for guidance,...   \n",
       "\n",
       "                                             MEANING  \n",
       "0                 میں اپنے والدین سے کیسے بات کروں ؟  \n",
       "1                             میں دوست کیسے بنائوں ؟  \n",
       "2                           میں اتنا اداس کیوں ہوں؟.  \n",
       "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...  \n",
       "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "path = Path(\"\")\n",
    "df = pd.read_excel('./parallel-corpus.xlsx')\n",
    "\n",
    "# Keep only the first two columns\n",
    "df = df.iloc[:, :2]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_urdu = Tokenizer()\n",
    "\n",
    "tokenizer_eng.fit_on_texts(df['SENTENCES'])\n",
    "tokenizer_urdu.fit_on_texts(df['MEANING'])\n",
    "\n",
    "eng_sequences = tokenizer_eng.texts_to_sequences(df['SENTENCES'])\n",
    "urdu_sequences = tokenizer_urdu.texts_to_sequences(df['MEANING'])\n",
    "\n",
    "# Pad sequences\n",
    "max_len_eng = max(len(seq) for seq in eng_sequences)\n",
    "max_len_urdu = max(len(seq) for seq in urdu_sequences)\n",
    "\n",
    "eng_sequences = pad_sequences(eng_sequences, maxlen=max_len_eng, padding='post')\n",
    "urdu_sequences = pad_sequences(urdu_sequences, maxlen=max_len_urdu, padding='post')\n",
    "\n",
    "# Vocabulary sizes\n",
    "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
    "vocab_size_urdu = len(tokenizer_urdu.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Prepare the Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_size = int(len(eng_sequences) * 0.8)\n",
    "eng_train, eng_val = eng_sequences[:train_size], eng_sequences[train_size:]\n",
    "urdu_train, urdu_val = urdu_sequences[:train_size], urdu_sequences[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Build the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, Embedding, Dense\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_len_eng,))\n",
    "encoder_embedding = Embedding(vocab_size_eng, 256)(encoder_inputs)\n",
    "encoder_rnn = SimpleRNN(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h = encoder_rnn(encoder_embedding)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_len_urdu,))\n",
    "decoder_embedding = Embedding(vocab_size_urdu, 256)(decoder_inputs)\n",
    "decoder_rnn = SimpleRNN(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _ = decoder_rnn(decoder_embedding, initial_state=state_h)\n",
    "decoder_dense = Dense(vocab_size_urdu, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the target data\n",
    "urdu_train_out = np.expand_dims(urdu_train, -1)\n",
    "urdu_val_out = np.expand_dims(urdu_val, -1)\n",
    "\n",
    "# Train the model\n",
    "model.fit([eng_train, urdu_train], urdu_train_out, \n",
    "          validation_data=([eng_val, urdu_val], urdu_val_out),\n",
    "          epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Evaluate and Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate a new sentence\n",
    "def translate_sentence(sentence):\n",
    "    sequence = tokenizer_eng.texts_to_sequences([sentence])\n",
    "    sequence = pad_sequences(sequence, maxlen=max_len_eng, padding='post')\n",
    "    states_value = encoder_model.predict(sequence)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer_urdu.word_index['starttoken']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = tokenizer_urdu.index_word[sampled_token_index]\n",
    "        \n",
    "        if sampled_word == 'endtoken' or len(decoded_sentence) > max_len_urdu:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_word\n",
    "        \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        states_value = [h]\n",
    "    \n",
    "    return decoded_sentence\n",
    "\n",
    "# Example translation\n",
    "print(translate_sentence(\"hello\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
