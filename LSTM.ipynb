{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCES</th>\n",
       "      <th>MEANING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I communicate with my parents?</td>\n",
       "      <td>میں اپنے والدین سے کیسے بات کروں ؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I make friends?’</td>\n",
       "      <td>میں دوست کیسے بنائوں ؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do I get so sad?’</td>\n",
       "      <td>میں اتنا اداس کیوں ہوں؟.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you’ve asked yourself such questions, you’r...</td>\n",
       "      <td>اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Depending on where you’ve turned for guidance,...</td>\n",
       "      <td>اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           SENTENCES  \\\n",
       "0             How can I communicate with my parents?   \n",
       "1                           How can I make friends?’   \n",
       "2                              Why do I get so sad?’   \n",
       "3  If you’ve asked yourself such questions, you’r...   \n",
       "4  Depending on where you’ve turned for guidance,...   \n",
       "\n",
       "                                             MEANING  \n",
       "0                 میں اپنے والدین سے کیسے بات کروں ؟  \n",
       "1                             میں دوست کیسے بنائوں ؟  \n",
       "2                           میں اتنا اداس کیوں ہوں؟.  \n",
       "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...  \n",
       "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_excel('./parallel-corpus.xlsx')\n",
    "\n",
    "# Keep only the first two columns\n",
    "df = df.iloc[:, :2]\n",
    "\n",
    "df.rename(columns = {'SENTENCES ':'SENTENCES'}, inplace = True)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 17:56:10.514685: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-12 17:56:10.515062: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-12 17:56:10.517525: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-12 17:56:10.524711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:476] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1728737770.534673   66811 cuda_dnn.cc:8312] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1728737770.538192   66811 cuda_blas.cc:1420] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-12 17:56:10.550127: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_urdu = Tokenizer()\n",
    "\n",
    "# Convert the 'SENTENCES' column to string type before fitting the tokenizer\n",
    "df['SENTENCES'] = df['SENTENCES'].astype(str)\n",
    "# Convert the 'MEANING' column to string type before fitting the tokenizer\n",
    "df['MEANING'] = df['MEANING'].astype(str)\n",
    "\n",
    "tokenizer_eng.fit_on_texts(df['SENTENCES'])\n",
    "tokenizer_urdu.fit_on_texts(df['MEANING'])\n",
    "\n",
    "eng_sequences = tokenizer_eng.texts_to_sequences(df['SENTENCES'])\n",
    "urdu_sequences = tokenizer_urdu.texts_to_sequences(df['MEANING'])\n",
    "\n",
    "# Pad sequences\n",
    "max_len_eng = max(len(seq) for seq in eng_sequences)\n",
    "max_len_urdu = max(len(seq) for seq in urdu_sequences)\n",
    "\n",
    "max_len = max(max_len_eng,max_len_urdu)\n",
    "\n",
    "eng_sequences = pad_sequences(eng_sequences, maxlen=max_len, padding='post')\n",
    "urdu_sequences = pad_sequences(urdu_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Vocabulary sizes\n",
    "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
    "vocab_size_urdu = len(tokenizer_urdu.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_size = int(len(eng_sequences) * 0.7)\n",
    "test_size = int(len(eng_sequences) * 0.15)\n",
    "\n",
    "# For English train, validation and test\n",
    "x_train, x_temp = eng_sequences[:train_size], eng_sequences[train_size:]\n",
    "x_test, x_val = x_temp[:test_size], x_temp[test_size:]\n",
    "\n",
    "# For Urdu train, validation and test\n",
    "y_train, y_temp = urdu_sequences[:train_size], urdu_sequences[train_size:]\n",
    "y_test, y_val = y_temp[:test_size], y_temp[test_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Build Model\n",
    "model = Sequential(\n",
    "    [\n",
    "        Embedding(input_dim=vocab_size_eng , output_dim=64),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dense(vocab_size_urdu, activation='softmax')\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21114, 938) (21114, 938)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train Model\n",
    "model.fit(x_train, y_train, epochs=25,validation_data=(x_val,y_val))\n",
    "\n",
    "\n",
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate function\n",
    "def translate(text):\n",
    "    sequence = tokenizer_eng.texts_to_sequences([text])\n",
    "    sequence = pad_sequences(sequence, maxlen=max_len_eng, padding='post')\n",
    "    prediction = model.predict(sequence)\n",
    "    predicted_sequence = np.argmax(prediction, axis=-1)\n",
    "    translated_text = ' '.join([tokenizer_urdu.index_word[idx] for idx in predicted_sequence[0] if idx != 0])\n",
    "    return translated_text\n",
    "\n",
    "# Example translation\n",
    "print(translate('hello'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
